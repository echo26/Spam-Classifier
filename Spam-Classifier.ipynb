{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".h1_cell, .just_text {\n",
       "    box-sizing: border-box;\n",
       "    padding-top:5px;\n",
       "    padding-bottom:5px;\n",
       "    font-family: \"Times New Roman\", Georgia, Serif;\n",
       "    font-size: 125%;\n",
       "    line-height: 22px; /* 5px +12px + 5px */\n",
       "    text-indent: 25px;\n",
       "    background-color: #fbfbea;\n",
       "    padding: 10px;\n",
       "    border-style: groove;\n",
       "}\n",
       "\n",
       "hr { \n",
       "    display: block;\n",
       "    margin-top: 0.5em;\n",
       "    margin-bottom: 0.5em;\n",
       "    margin-left: auto;\n",
       "    margin-right: auto;\n",
       "    border-style: inset;\n",
       "    border-width: 2px;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".h1_cell, .just_text {\n",
    "    box-sizing: border-box;\n",
    "    padding-top:5px;\n",
    "    padding-bottom:5px;\n",
    "    font-family: \"Times New Roman\", Georgia, Serif;\n",
    "    font-size: 125%;\n",
    "    line-height: 22px; /* 5px +12px + 5px */\n",
    "    text-indent: 25px;\n",
    "    background-color: #fbfbea;\n",
    "    padding: 10px;\n",
    "    border-style: groove;\n",
    "}\n",
    "\n",
    "hr { \n",
    "    display: block;\n",
    "    margin-top: 0.5em;\n",
    "    margin-bottom: 0.5em;\n",
    "    margin-left: auto;\n",
    "    margin-right: auto;\n",
    "    border-style: inset;\n",
    "    border-width: 2px;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "<center>\n",
    "Spam email Classifier using NLTK and Bayes Net\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Summary\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "    <p>I will get spam emails table from Kaggle and make a data model in json that will be used for spam classifier.</p>\n",
    "    <p><strong>Part 1, Data Loading: </strong>I first load the data using pandas</p>\n",
    "    <p><strong>Part 2, Sentence Wrangler: </strong>Then, I made a sentence_wrangler which classify useful words and useless words from each table using tokenizer. I removed the special characters using regular expression</p>\n",
    "    <p><strong>Part 3, bag of words: </strong>Make a bad of words using sentence_wrangler.</p>\n",
    "    <p><strong>Part 4, class counts: </strong>Prepare the class counts that will be used for the Naive Bayes.</p>\n",
    "    <p><strong>Part 5, Naive Bayes: </strong>Make the Naive Bayes function that will give spam and ham probabilities of each email.</p>\n",
    "    <p><strong>Part 6, Run and Accuracy: </strong>Run Naive Bayes for the whole table and get predictions for each email.</p>\n",
    "    <p><strong>Data Source: </strong> https://www.kaggle.com/llabhishekll/fraud-email-dataset.</p>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/edward/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/edward/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tree import Tree\n",
    "punctuation = string.punctuation\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "word_punct_tokenizer = WordPunctTokenizer()          #instantiate class\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "treeb_tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "1. Data Loading\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_table = pd.read_csv(\"fraud_email_.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Supply Quality China's EXCLUSIVE dimensions at...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>over. SidLet me know. Thx.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dear Friend,Greetings to you.I wish to accost ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MR. CHEUNG PUIHANG SENG BANK LTD.DES VOEUX RD....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not a surprising assessment from Embassy.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Class\n",
       "0  Supply Quality China's EXCLUSIVE dimensions at...      1\n",
       "1                         over. SidLet me know. Thx.      0\n",
       "2  Dear Friend,Greetings to you.I wish to accost ...      1\n",
       "3  MR. CHEUNG PUIHANG SENG BANK LTD.DES VOEUX RD....      1\n",
       "4          Not a surprising assessment from Embassy.      0"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11929"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spam_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_table['spam'] = spam_table.apply(lambda row: 'ham' if row['Class']==0 else 'spam', axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Class</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Supply Quality China's EXCLUSIVE dimensions at...</td>\n",
       "      <td>1</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>over. SidLet me know. Thx.</td>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dear Friend,Greetings to you.I wish to accost ...</td>\n",
       "      <td>1</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MR. CHEUNG PUIHANG SENG BANK LTD.DES VOEUX RD....</td>\n",
       "      <td>1</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not a surprising assessment from Embassy.</td>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Class  spam\n",
       "0  Supply Quality China's EXCLUSIVE dimensions at...      1  spam\n",
       "1                         over. SidLet me know. Thx.      0   ham\n",
       "2  Dear Friend,Greetings to you.I wish to accost ...      1  spam\n",
       "3  MR. CHEUNG PUIHANG SENG BANK LTD.DES VOEUX RD....      1  spam\n",
       "4          Not a surprising assessment from Embassy.      0   ham"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "2. Sentence Wrangling\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "word_punct_tokenizer = WordPunctTokenizer()          #instantiate class\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "treeb_tokenizer = TreebankWordTokenizer()            #instantiate class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords  # see more at http://xpo6.com/list-of-english-stop-words/\n",
    "swords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "punctuation = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def containsAnyPunc(st, punctuation):\n",
    "    ##help function to filter string containing punctuation\n",
    "    return 1 in [c in st for c in punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_wrangler(sentence, swords, punctuation):\n",
    "    \"\"\"\n",
    "    return list of lists\n",
    "    first list is containing meaningful or useful words and \n",
    "    the second one is from stop words or non-words (special characters).\n",
    "    \"\"\"\n",
    "    ans = [[],[]]\n",
    "    word_tokes = word_punct_tokenizer.tokenize(str(sentence).lower())\n",
    "    for unit in word_tokes:             ##unit small part of word like here, 's, my, people.\n",
    "        if containsAnyPunc(unit, punctuation) or unit in swords:\n",
    "            ans[1].append(unit)\n",
    "        elif re.findall(r'[\\W]', unit):   ##remove special charaters\n",
    "            ans[1].append(unit)\n",
    "        else:\n",
    "            ans[0].append(unit)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = 'Here\\'s is my whitelist - re pattern would be better. Extra credit if you do it'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['whitelist', 'pattern', 'would', 'better', 'extra', 'credit'],\n",
       " ['here', \"'\", 's', 'is', 'my', '-', 're', 'be', '.', 'if', 'you', 'do', 'it']]"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_wrangler(test_sentence, swords, punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "3. Bag of Words\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_words(table):\n",
    "    words_dictionary = dict()\n",
    "    empty = []\n",
    "    for i in range(len(table)):\n",
    "        sentence = str(table.loc[i, 'Text'])\n",
    "        spam_class = table.loc[i,'Class']\n",
    "        li = sentence_wrangler(sentence, swords, punctuation)\n",
    "        for word in li[0]:\n",
    "            if word in words_dictionary:\n",
    "                words_dictionary[word][spam_class] +=1\n",
    "            else:\n",
    "                words_dictionary[word] = [0,0]\n",
    "                words_dictionary[word][spam_class] +=1\n",
    "    return words_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = all_words(spam_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109450"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bag_of_words)  #unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'supply': [14, 126],\n",
       " 'quality': [21, 32],\n",
       " 'china': [337, 123],\n",
       " 'exclusive': [14, 12],\n",
       " 'dimensions': [4, 2],\n",
       " 'unbeatable': [0, 12],\n",
       " 'price': [12, 77],\n",
       " 'dear': [40, 1883],\n",
       " 'sir': [20, 1185],\n",
       " 'pleased': [28, 63]}"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first10pairs = {k: bag_of_words[k] for k in list(bag_of_words)[:10]}\n",
    "first10pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "4. Class count\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_count, spam_count = spam_table.groupby('Class').size()\n",
    "class_counts['class_count'] = ham_count, spam_count\n",
    "class_counts['spam_count'] = len(spam_table)\n",
    "class_counts['class_prob'] = ham_count / class_counts['spam_count'], spam_count / class_counts['spam_count']\n",
    "# class_counts['naked_count'] = spam_table.groupby(['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful_counts['naked_count'] = tweet_table.groupby(['label', 'hash_count']).size()[0][0],\\\n",
    "# tweet_table.groupby(['label', 'hash_count']).size()[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "5. Bayes Net\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes(text, count_dictionary, patc, bag_of_words, swords, punctuation):\n",
    "    ans_list = []\n",
    "    wrangle_li = sentence_wrangler(text, swords, punctuation)\n",
    "    word_list = list(word for word in wrangle_li[0] if not re.findall(patc, word))\n",
    "    for i in range(2):\n",
    "        case_i = count_dictionary['class_prob'][i]\n",
    "        num = 1\n",
    "        for word in word_list:\n",
    "            if word not in bag_of_words:\n",
    "                num *=1\n",
    "            else:\n",
    "                num *= bag_of_words[word][i] / count_dictionary['class_count'][i]\n",
    "        ans_list.append(num* case_i)\n",
    "    return tuple(ans_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "patc = r'^[0-9]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "patc = r'^[0-9\\W]*$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0, 1.7400676946902776e-262)\n",
      "1\n",
      "(2.7678419422269e-07, 0.0)\n",
      "0\n",
      "(0.0, 1.9102425502583288e-221)\n",
      "1\n",
      "(0.0, 4.510351283e-314)\n",
      "1\n",
      "(5.8160090278563125e-08, 1.0015286672506927e-07)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(naive_bayes(spam_table.loc[i, 'Text'], class_counts, patc, bag_of_words, swords, punctuation))\n",
    "    print(spam_table.loc[i, 'Class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "6. Run and Accuracy \n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "18.65878391265869\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "copytable = spam_table\n",
    "predictions = []\n",
    "val = 0\n",
    "for i,row in copytable.iterrows():\n",
    "    if i%1000 ==0:\n",
    "        print('did 1000')\n",
    "    pair = naive_bayes(row['Text'], class_counts, patc, bag_of_words, swords, punctuation)\n",
    "    predictions.append(pair.index(max(pair)))\n",
    "    \n",
    "end = time.time()\n",
    "print(end - start)  # in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    0\n",
      "2    1\n",
      "3    1\n",
      "4    0\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#build zipped\n",
    "actuals = spam_table['Class']\n",
    "print(actuals[:5])\n",
    "# for widx in range(len(actuals)):\n",
    "#     actuals[widx] = widx\n",
    "zipped = list(zip(predictions, actuals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_dictionary = {(1, 1):0, (1, 0):0, (0, 1):0, (0, 0):0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1, 1): 4314, (1, 0): 753, (0, 1): 873, (0, 0): 5989}"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for pair in zipped:\n",
    "    confusion_dictionary[pair] +=1\n",
    "confusion_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8636935199932937"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = (1.0*confusion_dictionary[(0,0)]+confusion_dictionary[(1,1)])/len(spam_table)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "Analyzing and save\n",
    "</h1>\n",
    "\n",
    "<div class=h1_cell>\n",
    "    <p>I got 86% accuracy.</p>\n",
    "    <p>I will save my bad_of_words into JSON to make a spam classifying program in the future.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('bag_of_words.txt', 'w') as file:\n",
    "    file.write(json.dumps(bag_of_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag2 = json.load(open(\"bag_of_words.txt\"))  # making sure I can read it in again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0', [92, 642]),\n",
       " ('00', [674, 1266]),\n",
       " ('000', [3, 2768]),\n",
       " ('0000', [0, 3]),\n",
       " ('00000', [0, 4]),\n",
       " ('000000', [0, 161]),\n",
       " ('00000000000000', [1, 0]),\n",
       " ('000000000066', [0, 4]),\n",
       " ('000000b2', [0, 1]),\n",
       " ('00000e25', [0, 1])]"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(bag2.items())[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
